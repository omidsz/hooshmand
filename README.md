
# ๐ RAG QA + TTS (ุณูุฏูุญูุฑ) โ ุชูุถุญ ุฎุทโุจูโุฎุท ููุชโุจูฺฉ
<div dir="rtl" style="text-align: right;">

ุงู ููุชโุจูฺฉ ฺฉ **RAG (Retrieval-Augmented Generation)** ุณุงุฏู ูโุณุงุฒุฏ:

**ุฌุฑุงู ฺฉู ฺฉุงุฑ:**

1) ุณูุฏ ุฑุง ุจู ฺูุฏ ยซุชฺฉู/ฺุงูฺฉยป ุชูุณู ูโฺฉูุฏ  
2) ุจุฑุง ูุฑ ฺุงูฺฉ embedding ูโุณุงุฒุฏ  
3) ุฏุงุฎู FAISS ุงูุฏฺฉุณ ูโฺฉูุฏ  
4) ุจุฑุง ุณูุงู ฺฉุงุฑุจุฑ ูุฒุฏฺฉโุชุฑู ฺุงูฺฉโูุง ุฑุง ูพุฏุง ูโฺฉูุฏ  
5) ุจุง ุขูโูุง ฺฉ prompt ูโุณุงุฒุฏ  
6) ุจุง ฺฉ ูุฏู ุณุจฺฉ (Flan-T5) ุฌูุงุจ ุชููุฏ ูโฺฉูุฏ  
7) ุฌูุงุจ ุฑุง ุจุง gTTS ุตูุช ูโฺฉูุฏ  
8) ููู ุฑุง ุฏุงุฎู ฺฉ ุฑุงุจุท Gradio ูุดุงู ูโุฏูุฏ  
</div>


## โ ุณููู 0 โ ูุตุจ ูพฺฉุฌโูุง + ุจุฑุฑุณ Torch/GPU

```python
!pip -q install faiss-cpu sentence-transformers transformers accelerate gradio gTTS
````

**ุงู ุณููู ูพฺฉุฌโูุง ูุงุฒู ุฑุง ูุตุจ ูโฺฉูุฏ:**
<div dir="rtl" style="text-align: right;">
* `faiss-cpu`: ุฌุณุชุฌู ุจุฑุฏุงุฑ ุณุฑุน (Vector Search) ุฑู CPU
* `sentence-transformers`: ุณุงุฎุช embedding ุจุฑุง ูุชู
* `transformers`, `accelerate`: ุงุฌุฑุง ูุฏูโูุง HuggingFace
* `gradio`: ุณุงุฎุช UI ูุจ ุจุฑุง ฺุช
* `gTTS`: ุชุจุฏู ูุชู ุจู ุตุฏุง ุจุง Google Text-to-Speech

> `-q` ุนู ุฎุฑูุฌ ูุตุจ ฺฉูโุญุฑูโุชุฑ ุจุงุดุฏ.
</div>


```python
import torch
print('torch:', torch.__version__)
print('cuda available:', torch.cuda.is_available())
```
<div dir="rtl" style="text-align: right;">
* `torch` ุฑุง ูุงุฑุฏ ูโฺฉูุฏ
* ูุณุฎูโ Torch ุฑุง ฺุงูพ ูโฺฉูุฏ
* ุจุฑุฑุณ ูโฺฉูุฏ ุขุง CUDA (GPU) ุฏุฑ ุฏุณุชุฑุณ ุงุณุช ุง ูู
</div>



## โ ุณููู 2 โ ุงููพูุฑุชโูุง ู ุงูุชุฎุงุจ ุฏุณุชฺฏุงู (CPU/GPU)

```python
import os
import re
import numpy as np
import faiss
from dataclasses import dataclass
```
<div dir="rtl" style="text-align: right;">
* `os`: ฺฉุงุฑ ุจุง ูุณุฑูุง/ูุงูโูุง (ุงูุฌุง ุฎู ุงุณุชูุงุฏู ูุดุฏู)
* `re`: regex ุจุฑุง ูพุงฺฉุณุงุฒ ูุชู
* `numpy`: ุขุฑุงูโูุง ุนุฏุฏ (embeddingูุง)
* `faiss`: ุณุงุฎุช index ู ุฌุณุชุฌู ุดุจุงูุช
* `dataclass`: ุงููพูุฑุช ุดุฏู ูู ุงุณุชูุงุฏู ูุดุฏู (ูโุชูุงูุฏ ุญุฐู ุดูุฏ)
</div>


```python
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
```
<div dir="rtl" style="text-align: right;">
* `SentenceTransformer`: ูุฏู embedding
* `AutoTokenizer` ู `AutoModelForSeq2SeqLM`: ุชูฺฉูุงุฒุฑ ู ูุฏู ุชููุฏ ูุชู (seq2seq)
* ุฏูุจุงุฑู `torch` ุงููพูุฑุช ุดุฏู (ุชฺฉุฑุงุฑ ุงุณุช ูู ูุดฺฉู ุงุฌุงุฏ ููโฺฉูุฏ)
</div>
```python
from gtts import gTTS
import gradio as gr
```
<div dir="rtl" style="text-align: right;">
* `gTTS`: ูุชู โ mp3
* `gradio`: UI
</div>


```python
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
print('DEVICE:', DEVICE)
```

* ุงฺฏุฑ GPU ุจุงุดุฏ ุฑู `cuda` ูโุฑูุฏ ูฺฏุฑูู `cpu`
* ุฏุณุชฺฏุงู ุงูุชุฎุงุจ ุฑุง ฺุงูพ ูโฺฉูุฏ



## โ ุณููู 3 โ ูุชู ุณูุฏ + ุชุงุจุน ุฎูุงูุฏู ูุงู txt

```python
DOCUMENT_TEXT = """
""".strip()
```
<div dir="rtl" style="text-align: right;">

* ูุชุบุฑ ุงุตู ุณูุฏ
* ูุนูุงู ุฎุงู ุงุณุช
* `.strip()` ูุงุตููโูุง ุงุจุชุฏุง/ุงูุชูุง ุฑุง ุญุฐู ูโฺฉูุฏ
</div>


```python
def load_txt_from_path(path: str) -> str:
    with open(path, 'r', encoding='utf-8') as f:
        return f.read()
```
<div dir="rtl" style="text-align: right;">
* ุชุงุจุน ุณุงุฏู ุจุฑุง ุฎูุงูุฏู ูุงู ูุชู UTF-8
* `with open(...)` ุนู ูุงู ุจุนุฏ ุงุฒ ุฎูุงูุฏู ุฎูุฏฺฉุงุฑ ุจุณุชู ูโุดูุฏ
</div>


```python
print('Document chars:', len(DOCUMENT_TEXT))
```
<div dir="rtl" style="text-align: right;">
* ุชุนุฏุงุฏ ฺฉุงุฑุงฺฉุชุฑูุง ุณูุฏ ูุนู ุฑุง ฺุงูพ ูโฺฉูุฏ (ุงูุฌุง ุงุญุชูุงูุงู 0)
</div>


## โ ุณููู 5 โ ูุฑูุงูโุณุงุฒ ูุชู + ฺุงูฺฉโฺฉุฑุฏู

### 1) ูุฑูุงูโุณุงุฒ ูุชู

```python
def normalize_text(text: str) -> str:
    text = text.replace('\u200c', ' ')  # ZWNJ
    text = re.sub(r"\s+", " ", text).strip()
    return text
```
<div dir="rtl" style="text-align: right;">
* `\u200c` ููุงู **ููโูุงุตูู (ZWNJ)** ุงุณุชุ ุขู ุฑุง ุจู ูุงุตูู ุชุจุฏู ูโฺฉูุฏ ุชุง ูุชู ฺฉููุงุฎุช ุดูุฏ
* `re.sub(r"\s+", " ", text)`: ูุฑ ุชุนุฏุงุฏ whitespace (ูุงุตูู/ุฎุท ุฌุฏุฏ/ุชุจ) โ ฺฉ ูุงุตูู
* `strip()`: ุญุฐู ูุงุตููโูุง ุงุจุชุฏุง ู ุงูุชูุง
</div>


### 2) ฺุงูฺฉโฺฉุฑุฏู ูุชู

```python
def chunk_text(text: str, chunk_size: int = 450, overlap: int = 80):
    """Chunking ุณุงุฏู ุจุฑ ุงุณุงุณ ุชุนุฏุงุฏ ฺฉุงุฑุงฺฉุชุฑ
    chunk_size ู overlap ูุงุจู ุชุบุฑ ูุณุชูุฏ
    """
```
<div dir="rtl" style="text-align: right;">
* ุชฺฉูโุชฺฉู ฺฉุฑุฏู ูุชู ุจุฑ ุงุณุงุณ **ุชุนุฏุงุฏ ฺฉุงุฑุงฺฉุชุฑ**
* `chunk_size`: ุทูู ูุฑ ุชฺฉู
* `overlap`: ูููพูุดุงู ุจู ุชฺฉูโูุง (ุจุฑุง ุงูฺฉู ูุทุงูุจ ูุฑุฒ ุงุฒ ุฏุณุช ูุฑููุฏ)
</div>


```python
    text = normalize_text(text)
    if not text:
        return []
```
<div dir="rtl" style="text-align: right;">
* ุงุจุชุฏุง ูุชู ุฑุง ูุฑูุงู ูโฺฉูุฏ
* ุงฺฏุฑ ุฎุงู ุจูุฏ ูุณุช ุฎุงู ุจุฑูโฺฏุฑุฏุงูุฏ
</div>


```python
    chunks = []
    start = 0
    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
```
<div dir="rtl" style="text-align: right;">
* ุงุฒ `start` ุชุง `end` ฺฉ ุจุฑุด ูโฺฏุฑุฏ
* `min` ุจุฑุง ุงูฺฉู ุงุฒ ุทูู ูุชู ุฌููุชุฑ ูุฑูุฏ
* ุงฺฏุฑ ุชฺฉู ุฎุงู ูุจูุฏ ุจู ูุณุช ุงุถุงูู ูโฺฉูุฏ
</div>


```python
        start = end - overlap
        if start < 0:
            start = 0
        if end == len(text):
            break
```
<div dir="rtl" style="text-align: right;">
* ุจุฑุง ฺุงูฺฉ ุจุนุฏุ `start` ุฑุง ุนูุจโุชุฑ ูโุจุฑุฏ ุชุง overlap ุงุฌุงุฏ ุดูุฏ
* ุงฺฏุฑ ููู ุดุฏ ุตูุฑ ูโฺฉูุฏ
* ุงฺฏุฑ ุจู ุงูุชูุง ูุชู ุฑุณุฏูุ ุญููู ุชูุงู ูโุดูุฏ
</div>

```python
    return chunks
```

* ุฎุฑูุฌ: ูุณุช ฺุงูฺฉโูุง



```python
chunks = chunk_text(DOCUMENT_TEXT, chunk_size=450, overlap=80)
print('Num chunks:', len(chunks))
print('Sample chunk:\n', chunks[0][:300] if chunks else 'EMPTY')
```
<div dir="rtl" style="text-align: right;">
* ฺุงูฺฉโูุง ุณูุฏ ูุนู ุฑุง ูโุณุงุฒุฏ
* ุชุนุฏุงุฏุดุงู ุฑุง ฺุงูพ ูโฺฉูุฏ
* ุงฺฏุฑ ฺุงูฺฉ ูุฌูุฏ ุฏุงุดุช ณฐฐ ฺฉุงุฑุงฺฉุชุฑ ุงูู ฺุงูฺฉ ุงูู ุฑุง ูุดุงู ูโุฏูุฏ
</div>

## โ ุณููู 7 โ embedding ฺฏุฑูุชู ุงุฒ ฺุงูฺฉโูุง

```python
EMBED_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
embedder = SentenceTransformer(EMBED_MODEL_NAME)
```
<div dir="rtl" style="text-align: right;">
* ุงุณู ูุฏู embedding ฺูุฏุฒุจุงูู (ุจุฑุง ูุงุฑุณ ูู ููุงุณุจ)
* ูุฏู ุฑุง ููุฏ ูโฺฉูุฏ
</div>


```python
def embed_texts(texts):
    vecs = embedder.encode(texts, convert_to_numpy=True, show_progress_bar=True, normalize_embeddings=True)
    return vecs.astype('float32')
```
<div dir="rtl" style="text-align: right;">
* `encode`: ูุชูโูุง ุฑุง ุจู ุจุฑุฏุงุฑ ุชุจุฏู ูโฺฉูุฏ
* `convert_to_numpy=True`: ุฎุฑูุฌ numpy array
* `show_progress_bar=True`: ููุงุด ููุงุฑ ูพุดุฑูุช
* `normalize_embeddings=True`: ูุฑูุงูโุณุงุฒ ุจุฑุฏุงุฑูุง (ุจุฑุง ุดุจุงูุช ุจูุชุฑ)
* `astype('float32')`: FAISS ูุนูููุงู float32 ุฏูุณุช ุฏุงุฑุฏ
</div>


```python
chunk_embeddings = embed_texts(chunks) if chunks else np.zeros((0, 384), dtype='float32')
print('Embeddings shape:', chunk_embeddings.shape)
```
<div dir="rtl" style="text-align: right;">

* ุงฺฏุฑ ฺุงูฺฉ ุฏุงุฑู embedding ูโฺฏุฑุฏ
* ุงฺฏุฑ ูุฏุงุฑู ุขุฑุงู ุฎุงู ุจุง ุดฺฉู `(0, 384)` ูโุณุงุฒุฏ (ณธด ุงุจุนุงุฏ ุงู ูุฏู ุงุณุช)
* ุดฺฉู embeddingูุง ุฑุง ฺุงูพ ูโฺฉูุฏ
</div>


## โ ุณููู 8 โ ุณุงุฎุช ุงูุฏฺฉุณ FAISS

```python
def build_faiss_index(embeddings: np.ndarray):
    if embeddings.size == 0:
        return None
```

* ุงฺฏุฑ embedding ุฎุงู ุจุงุดุฏุ index ุณุงุฎุชู ููโุดูุฏ



```python
    dim = embeddings.shape[1]
    index = faiss.IndexFlatIP(dim)
    index.add(embeddings)
    return index
```
<div dir="rtl" style="text-align: right;">
* `dim`: ุชุนุฏุงุฏ ุงุจุนุงุฏ ุจุฑุฏุงุฑ
* `IndexFlatIP`: ุงูุฏฺฉุณ ุณุงุฏู ุจุง ูุนุงุฑ **Inner Product**
  ฺูู embeddingูุง ูุฑูุงู ุดุฏูโุงูุฏุ `inner product โ cosine similarity`
* `add`: ููู embeddingูุง ุฑุง ูุงุฑุฏ ุงูุฏฺฉุณ ูโฺฉูุฏ
</div>


```python
index = build_faiss_index(chunk_embeddings)
print('FAISS index ready:', index is not None)
```

* ุงูุฏฺฉุณ ุณุงุฎุชู ูโุดูุฏ ู ุขูุงุฏู ุจูุฏูุด ฺุงูพ ูโุดูุฏ



## โ ุณููู 10 โ ุจุงุฒุงุจ Top-k ฺุงูฺฉโูุง ูุฑุชุจุท

```python
def retrieve_top_k(query: str, k: int = 4):
    if index is None or not chunks:
        return []
```

* ุงฺฏุฑ ุงูุฏฺฉุณ/ฺุงูฺฉ ูุฏุงุฑูุ ุฎุฑูุฌ ุฎุงู



```python
    q_emb = embed_texts([query])
    scores, ids = index.search(q_emb, k)
```
<div dir="rtl" style="text-align: right;">
* embedding ุณูุงู ุฑุง ูโุณุงุฒุฏ (ฺฉ ุณูุงู โ ฺฉ ุจุฑุฏุงุฑ)
* `search`: ูุฒุฏฺฉโุชุฑู `k` ุจุฑุฏุงุฑ ุฑุง ูโุฏูุฏ

  * `ids`: ุงูุฏุณ ฺุงูฺฉโูุง
  * `scores`: ุงูุชุงุฒ ุดุจุงูุช
</div>


```python
    ids = ids[0].tolist()
    scores = scores[0].tolist()
```
<div dir="rtl" style="text-align: right;">
* ฺูู ุฎุฑูุฌ ุฏูุจุนุฏ ุงุณุช (batch)ุ ุณุทุฑ ุงูู ุฑุง ูโฺฏุฑุฏ ู ุจู ูุณุช ุชุจุฏู ูโฺฉูุฏ
</div>


```python
    results = []
    for i, s in zip(ids, scores):
        if i == -1:
            continue
        results.append((chunks[i], float(s), i))
    return results
```
<div dir="rtl" style="text-align: right;">
* ุงฺฏุฑ `-1` ุจูุฏ ูุชุฌู ูุงูุนุชุจุฑ ุงุณุช
* ุฎุฑูุฌ ูุฑ ูุชุฌู: `(ูุชู ฺุงูฺฉุ ุงูุชุงุฒุ ุงูุฏุณ)`
</div>


```python
test_q = "ููุถูุน ุณูุฏ ฺุณุชุ"
print(retrieve_top_k(test_q, k=3)[:1])
```
<div dir="rtl" style="text-align: right;">
* ุชุณุช: ุณูุงู ูโูพุฑุณุฏ ู ฑ ูุชุฌู ุงูู ุฑุง ฺุงูพ ูโฺฉูุฏ
</div>

<div dir="rtl" style="text-align: right;">
## โ ุณููู 12 โ ุณุงุฎุช prompt ุจุฑุง ูุฏู ุฒุจุงู

</div>

```python
def build_prompt(context_chunks, question: str) -> str:
    context = "\n\n".join([f"[{i}] {c}" for i, c in enumerate(context_chunks, start=1)])
```

* ฺุงูฺฉโูุง ุงูุชุฎุงุจโุดุฏู ุฑุง ุดูุงุฑูโฺฏุฐุงุฑ ู ฺฉูพุงุฑฺู ูโฺฉูุฏ



```python
    prompt = f"""
You are a QA assistant.

RULES:
1) Answer ONLY using the provided CONTEXT.
2) If the answer is not in the context, say exactly: "ุงุทูุงุนุงุช ฺฉุงู ุฏุฑ ูุชู ููุฌูุฏ ูุณุช."
3) Keep the answer concise and well-structured.

CONTEXT:
{context}

QUESTION:
{question}

ANSWER (in Persian):
""".strip()
```
<div dir="rtl" style="text-align: right;">
* prompt ุงูฺฏูุณ ุงุณุช ูู ูโฺฏูุฏ ุฌูุงุจ **ูุงุฑุณ** ุจุงุดุฏ
* ูุงููู ููู: ููุท ุงุฒ `CONTEXT` ุงุณุชูุงุฏู ฺฉูุ ุงฺฏุฑ ูุจูุฏ ุฏููุงู ููุงู ุฌููู ุฑุง ุจฺฏู

</div>


```python
    return prompt
```
<div dir="rtl" style="text-align: right;">
* prompt ููุง ุจุฑูโฺฏุฑุฏุฏ
</div>


## โ ุณููู 14 โ ููุฏ ูุฏู ุชููุฏ ูพุงุณุฎ + ุชููุฏ ูพุงุณุฎ

```python
LLM_NAME = "google/flan-t5-small"
tokenizer = AutoTokenizer.from_pretrained(LLM_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(LLM_NAME)
model.to(DEVICE)
```
<div dir="rtl" style="text-align: right;">
* ูุฏู ุณุจฺฉ `flan-t5-small` ุฑุง ููุฏ ูโฺฉูุฏ
* ุชูฺฉูุงุฒุฑ ู ูุฏู ุฑุง ูโุณุงุฒุฏ
* ุฑู CPU ุง GPU ูโุจุฑุฏ
</div>


```python
def generate_answer(prompt: str, max_new_tokens: int = 180):
    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=1024).to(DEVICE)
```
<div dir="rtl" style="text-align: right;">
* prompt ุฑุง ุชูฺฉูุงุฒ ูโฺฉูุฏ
* `truncation=True`: ุงฺฏุฑ ุทููุงู ุดุฏ ูุทุน ฺฉูุฏ
* `max_length=1024`: ุณูู ุทูู ูุฑูุฏ
* ุฏุงุฏูโูุง ุฑุง ุฑู `DEVICE` ูโุจุฑุฏ
</div>


```python
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            num_beams=4,
            do_sample=False,
        )
```
<div dir="rtl" style="text-align: right;">
* `no_grad`: inference ุจุฏูู ูุญุงุณุจู ฺฏุฑุงุฏุงู
* `generate`:

  * `max_new_tokens`: ุญุฏุงฺฉุซุฑ ุทูู ุฌูุงุจ
  * `num_beams=4`: beam search ุจุฑุง ุฌูุงุจ ุจูุชุฑ
  * `do_sample=False`: ุชุตุงุฏู ูุณุช (ูพุงุฏุงุฑุชุฑ)
</div>


```python
    text = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()
    return text
```

* ุชุจุฏู ุฎุฑูุฌ ุชูฺฉูโูุง ุจู ูุชู ู ุจุฑฺฏุดุช ุฏุงุฏู ุฌูุงุจ



### ุชุณุช ุณุฑุน

```python
if chunks:
    r = retrieve_top_k("ุณูุฏ ุฏุฑุจุงุฑู ฺุณุชุ", k=4)
    ctx = [x[0] for x in r]
    p = build_prompt(ctx, "ุณูุฏ ุฏุฑุจุงุฑู ฺุณุชุ")
    print(generate_answer(p))
else:
    print('Document is empty. Paste or upload a .txt first.')
```
<div dir="rtl" style="text-align: right;">

* ุงฺฏุฑ ุณูุฏ ุฏุงุฑู: retrieval โ prompt โ ุฌูุงุจ ฺุงูพ ูโุดูุฏ
* ุงฺฏุฑ ูุฏุงุฑู: ูพุงู ุณูุฏ ุฎุงู ุงุณุช
</div>


## โ ุณููู 16 โ ูุชู ุจู ุตุฏุง (TTS)

```python
from gtts import gTTS
```

* ุงููพูุฑุช ุชฺฉุฑุงุฑ (ุงุดฺฉุงู ูุฏุงุฑุฏ)



```python
def text_to_speech(text, out_path="answer.mp3", lang="en"):
    text = (text or "").strip()
    if not text:
        return None
```
<div dir="rtl" style="text-align: right;">
* ูุชู ุฑุง ุงูู ูโฺฉูุฏ (ุงฺฏุฑ `None` ุจูุฏุ ุฑุดุชู ุฎุงู)
* ุงฺฏุฑ ุฎุงู ุจูุฏ `None` ุจุฑูโฺฏุฑุฏุงูุฏ
</div>


```python
    gTTS(text=text, lang=lang).save(out_path)
    return out_path
```
<div dir="rtl" style="text-align: right;">
* ุจุง gTTS ูุงู mp3 ูโุณุงุฒุฏ ู ูุณุฑุด ุฑุง ุจุฑูโฺฏุฑุฏุงูุฏ

> โ๏ธ ูฺฉุชู: `lang="en"` ุงุณุชุ ุงฺฏุฑ ุฌูุงุจ ูุงุฑุณ ุงุณุช ุจูุชุฑ ุงุณุช `"fa"` ุจุงุดุฏ.
</div>


## โ ุณููู 18 โ ุชุงุจุน ุงุตู RAG

```python
def rag_answer(question: str, top_k: int = 4):
    if not DOCUMENT_TEXT.strip() or not chunks:
        return "ุงุจุชุฏุง ูุชู ุณูุฏ ุฑุง ูุงุฑุฏ ฺฉูุฏ (Paste ุง ูุงู .txt).", []
```
<div dir="rtl" style="text-align: right;">
* ุงฺฏุฑ ุณูุฏ/ฺุงูฺฉ ูุฏุงุฑู: ูพุงู ุฎุทุง + ูุณุช ุฎุงู
</div>


```python
    retrieved = retrieve_top_k(question, k=top_k)
    context_chunks = [c for (c, s, idx) in retrieved]
    prompt = build_prompt(context_chunks, question)
    answer = generate_answer(prompt)
    return answer, retrieved
```
<div dir="rtl" style="text-align: right;">

* retrieval ุงูุฌุงู ูโุฏูุฏ
* ููุท ูุชู ฺุงูฺฉโูุง ุฑุง ุฌุฏุง ูโฺฉูุฏ
* prompt ูโุณุงุฒุฏ
* ุฌูุงุจ ุชููุฏ ูโฺฉูุฏ
* ุฎุฑูุฌ: `(answer, retrieved_details)`
</div>


## โ ุณููู 20 โ ุจุงุฒุณุงุฒ pipeline + ุฑุงุจุท Gradio

### 1) ุจุงุฒุณุงุฒ ุงูุฏฺฉุณ ุจุง ุณูุฏ ุฌุฏุฏ

```python
def rebuild_pipeline_with_new_doc(doc_text: str, chunk_size: int = 450, overlap: int = 80):
    global DOCUMENT_TEXT, chunks, chunk_embeddings, index
```

* ฺูู ูโุฎูุงูุฏ ูุชุบุฑูุง ุณุฑุงุณุฑ ุฑุง ุชุบุฑ ุฏูุฏุ `global` ูโฺฏุฐุงุฑุฏ



```python
    DOCUMENT_TEXT = (doc_text or "").strip()
    chunks = chunk_text(DOCUMENT_TEXT, chunk_size=chunk_size, overlap=overlap)
```

* ูุชู ุณูุฏ ุฑุง ุฐุฎุฑู ูโฺฉูุฏ
* ฺุงูฺฉ ูโฺฉูุฏ



```python
    if chunks:
        chunk_embeddings = embed_texts(chunks)
        index = build_faiss_index(chunk_embeddings)
    else:
        chunk_embeddings = np.zeros((0, 384), dtype='float32')
        index = None
    return f"โ ุณูุฏ ุจุงุฑฺฏุฐุงุฑ ุดุฏ. ุชุนุฏุงุฏ chunk: {len(chunks)}"
```
<div dir="rtl" style="text-align: right;">
* ุงฺฏุฑ ฺุงูฺฉ ูุณุช: embedding โ index
* ุงฺฏุฑ ูุณุช: ููู ฺุฒ ุฎุงู
* ูพุงู ูุถุนุช ุจุฑูโฺฏุฑุฏุงูุฏ
</div>


### 2) ุฎูุงูุฏู ูุงู ุขูพููุฏ Gradio

```python
def read_uploaded_file(file_obj):
    if file_obj is None:
        return ""
```
<div dir="rtl" style="text-align: right;">
ุณูพุณ ููุนโูุง ูุฎุชูู ฺฉู Gradio ููฺฉู ุงุณุช ุจุฑฺฏุฑุฏุงูุฏ ุฑุง ูพูุดุด ูโุฏูุฏ:
</div>

```python
    if isinstance(file_obj, str):
        path = file_obj
    elif hasattr(file_obj, "name"):
        path = file_obj.name
    elif isinstance(file_obj, dict) and "path" in file_obj:
        path = file_obj["path"]
    elif hasattr(file_obj, "path"):
        path = file_obj.path
    else:
        raise TypeError(...)
```

ุจุนุฏ ูุงู ุฑุง ูโุฎูุงูุฏ:

```python
    with open(path, "r", encoding="utf-8") as f:
        return f.read()
```


<div dir="rtl" style="text-align: right;">
### 3) ุชุงุจุน ฺุช (ูุฑูุฏ ฺฉุงุฑุจุฑ โ ุฌูุงุจ + ุณูุฑุณโูุง + ุตูุช)
</div>

```python
def chat_fn(message, history, top_k, chunk_size, overlap):
    if not (DOCUMENT_TEXT and DOCUMENT_TEXT.strip()):
        return (history or []), "โ ุงุจุชุฏุง ุณูุฏ ุฑุง ูุงุฑุฏ ฺฉูุฏ.", None
```
<div dir="rtl" style="text-align: right;">
* ุงฺฏุฑ ุณูุฏ ูุณุชุ ณ ุฎุฑูุฌ ูโุฏูุฏ ฺูู UI ุณู ุฎุฑูุฌ ุฏุงุฑุฏ:

  1. history ฺุช
  2. markdown ุณูุฑุณโูุง
  3. audio (ูฺ)

</div>


```python
    answer, retrieved = rag_answer(message, top_k=int(top_k))
    answer = (answer or "").strip()
```
* ุฌูุงุจ RAG
* ุชูุฒฺฉุงุฑ



```python
    if not answer:
        answer = "ูุชุฃุณูุงูู ูพุงุณุฎ ูุงุจู ุชููุฏ ูุณุช..."
```

* ุงฺฏุฑ ุฎุงู ุจูุฏ ูพุงู fallback ูโุฏูุฏ



```python
    audio_path = text_to_speech(answer, out_path="answer.mp3", lang="en")
```

* ุฌูุงุจ ุฑุง ุตูุช ูโฺฉูุฏ

> ุจูุชุฑ: `lang="fa"` ุงฺฏุฑ ูุงุฑุณ ุงุณุช



```python
    sources_md = "\n\n".join([
        f"**Chunk #{idx} | score={score:.3f}**\n\n{chunk[:700]}"
        for (chunk, score, idx) in retrieved
    ])
```

* ฺุงูฺฉโูุง ุจุงุฒุงุจโุดุฏู ุฑุง ุจู markdown ุชุจุฏู ูโฺฉูุฏ (ุชุง ทฐฐ ฺฉุงุฑุงฺฉุชุฑ ุงุฒ ูุฑ ฺุงูฺฉ)



```python
    history = (history or []) + [(message, answer)]
    return history, sources_md, audio_path
```

* history ุฑุง ุขูพุฏุช ูโฺฉูุฏ ู ุฎุฑูุฌโูุง ุฑุง ุจุฑูโฺฏุฑุฏุงูุฏ



### 4) ุณุงุฎุช UI ุจุง Gradio

```python
with gr.Blocks() as demo:
    gr.Markdown("# RAG QA + TTS (ุณูุฏูุญูุฑ)")
```

* ฺฉ ุงูพ Gradio ูโุณุงุฒุฏ

**ุจุฎุด ูุฑูุฏ ุณูุฏ:**

* `doc_paste = gr.Textbox(...)`
* `doc_file = gr.File(...)`
* `chunk_size = gr.Slider(...)`
* `overlap = gr.Slider(...)`
* `load_btn = gr.Button(...)`
* `load_status = gr.Textbox(...)`

**ุจุฎุด ฺุช:**

* `chatbot = gr.Chatbot(...)`
* `msg = gr.Textbox(...)`
* `top_k = gr.Slider(...)`
* `sources = gr.Markdown(...)`
* `audio = gr.Audio(..., type="filepath")`


### 5) ุชุงุจุน `on_load` (ููุช paste/file/slider ุชุบุฑ ฺฉูุฏ)

```python
def on_load(doc_text, file_obj, chunk_size, overlap):
  if doc_text and doc_text.strip():
      chosen_text = doc_text
  elif file_obj is not None:
      chosen_text = read_uploaded_file(file_obj)
  else:
      chosen_text = ""
```
<div dir="rtl" style="text-align: right;">
* ุงูููุช ุจุง paste ุงุณุชุ ุงฺฏุฑ ุฎุงู ุจูุฏ ุงุฒ ูุงู ูโุฎูุงูุฏ
</div>


```python
  if not (chosen_text and chosen_text.strip()):
      return "โ ุณูุฏ ุฎุงู ุงุณุช..."
  return rebuild_pipeline_with_new_doc(chosen_text, int(chunk_size), int(overlap))
```

* ุงฺฏุฑ ุณูุฏ ุฎุงู ุจูุฏ ูพุงู ูโุฏูุฏ
* ูฺฏุฑูู ุงูุฏฺฉุณ ุฑุง ูโุณุงุฒุฏ



### 6) ุงุชุตุงู ุฑูุฏุงุฏูุง (Event Handlers)

```python
load_btn.click(on_load, inputs=[...], outputs=[load_status])
doc_paste.change(on_load, ...)
doc_file.change(on_load, ...)
chunk_size.change(on_load, ...)
overlap.change(on_load, ...)
```

* ุจุง ูุฑ ุชุบุฑุ ุงูุฏฺฉุณ ุฏูุจุงุฑู ุณุงุฎุชู ูโุดูุฏ


**ุงุฑุณุงู ูพุงู ฺุช:**

```python
msg.submit(chat_fn, inputs=[msg, chatbot, top_k, chunk_size, overlap], outputs=[chatbot, sources, audio])
msg.submit(lambda: "", None, msg)
```
<div dir="rtl" style="text-align: right;">
* submit ุงูู: ุฌูุงุจ ุฑุง ูโฺฏุฑุฏ
* submit ุฏูู: textbox ูพุงู ุฑุง ุฎุงู ูโฺฉูุฏ
</div>


### ุงุฌุฑุง ุงูพ

```python
demo.launch(share=True, debug=True)
```

* ุงูพ ุฑุง ุงุฌุฑุง ูโฺฉูุฏ
* `share=True`: ููฺฉ ุนููู ูููุช ูโุฏูุฏ
* `debug=True`: ุฎุทุงูุง ุฑุง ุฏููโุชุฑ ฺุงูพ ูโฺฉูุฏ


<div dir="rtl" style="text-align: right;">


ูุงูพุฑูพุงุฑุงูุชุฑูุง ุงุตู ู ุชุฃุซุฑุงุช ุขููุง:
1. Chunk Size (ุงูุฏุงุฒู ูุทุนุงุช ูุชู)
python
chunk_size: int = 450  # ูพุดโูุฑุถ
ุชุฃุซุฑ:

ฺฉูฺฺฉ ุจูุฏู: ุฏูุช ุจุงุฒุงุจ ุจุงูุงุชุฑุ ุงูุง ุงุทูุงุนุงุช ุฒููู (context) ูุญุฏูุฏุชุฑ

ุจุฒุฑฺฏ ุจูุฏู: ุงุทูุงุนุงุช ุฒููู ุจุดุชุฑุ ุงูุง ููฺฉู ุงุณุช ููุฒ ุงูุฒุงุด ุงุจุฏ

ุจููู: ูุนูููุงู ุจู 300-600 ฺฉุงุฑุงฺฉุชุฑ ุจุฑุง QA ููุงุณุจ ุงุณุช

2. Overlap (ููโูพูุดุงู ูุทุนุงุช)
python
overlap: int = 80  # ูพุดโูุฑุถ
ุชุฃุซุฑ:

ุงูุฒุงุด: ุฌููฺฏุฑ ุงุฒ ูุทุน ุดุฏู ุฌููุงุช ุฏุฑ ูุฑุฒ chunkูุงุ ุจูุจูุฏ ูพูุณุชฺฏ ูุชู

ุฒุงุฏ ุจูุฏู: ุฐุฎุฑูโุณุงุฒ ุชฺฉุฑุงุฑ ู ุงูุฒุงุด ูุฒูู ูุญุงุณุจุงุช

ุจููู: ูุนูููุงู 10-20% ุงุฒ chunk_size

3. Top-k (ุชุนุฏุงุฏ ูุทุนุงุช ุจุงุฒุงุจโุดุฏู)
python
top_k: int = 4  # ูพุดโูุฑุถ
ุชุฃุซุฑ:

ฺฉู ุจูุฏู (ูุซูุงู 1-2): ูพุงุณุฎ ุณุฑุนโุชุฑุ ุงูุง ููฺฉู ุงุณุช ุงุทูุงุนุงุช ฺฉุงู ูุจุงุดุฏ

ุฒุงุฏ ุจูุฏู (ูุซูุงู 8-10): ุงุทูุงุนุงุช ุจุดุชุฑ ุจุฑุง LLMุ ุงูุง ุงุญุชูุงู ููุฒ ุงูุฒุงุด ูโุงุจุฏ

ุจููู: ูุนูููุงู ุจู 3-5 ุจุฑุง ุชุนุงุฏู ููุงุณุจ

4. Max New Tokens (ุญุฏุงฺฉุซุฑ ุทูู ูพุงุณุฎ)
python
max_new_tokens: int = 180
ุชุฃุซุฑ:

ฺฉู ุจูุฏู: ูพุงุณุฎโูุง ฺฉูุชุงู ู ูุฎุชุตุฑ

ุฒุงุฏ ุจูุฏู: ูพุงุณุฎโูุง ุทููุงูโุชุฑุ ุงูุง ููฺฉู ุงุณุช ุดุงูู ุงุทูุงุนุงุช ูุงูุฑุจูุท ุดูุฏ

ุจููู: ุจุฑุง QA ูุนููู 100-200 ฺฉุงู ุงุณุช

5. Num Beams (ุฌุณุชุฌู beam ุฏุฑ ุชููุฏ)
python
num_beams: int = 4
ุชุฃุซุฑ:

ุงูุฒุงุด: ฺฉูุช ูพุงุณุฎ ุจูุชุฑุ ุงูุง ุณุฑุนุช ุชููุฏ ฺฉุงูุด ูโุงุจุฏ

ฺฉุงูุด: ูพุงุณุฎ ุณุฑุนโุชุฑุ ุงูุง ููฺฉู ุงุณุช ฺฉูุช ุงูุช ฺฉูุฏ

ุจููู: ูุนูููุงู 4-6 ุจุฑุง ุชุนุงุฏู ููุงุณุจ

ุชุฃุซุฑุงุช ฺฉู ุชุบุฑ ูุงูพุฑูพุงุฑุงูุชุฑูุง:
ูุซุจุช:
ุงูุฒุงุด ุฏูุช: chunk_size ููุงุณุจ + overlap ฺฉุงู

ูพุงุณุฎ ฺฉุงููโุชุฑ: top_k ุจุดุชุฑ + max_new_tokens ููุงุณุจ

ฺฉูุช ุจูุชุฑ ูพุงุณุฎ: num_beams ุจุดุชุฑ

ููู:
ฺฉุงูุด ุณุฑุนุช: ุงูุฒุงุด top_kุ num_beamsุ ุง ฺฉุงูุด chunk_size (ุชุนุฏุงุฏ chunk ุจุดุชุฑ)

ุงูุฒุงุด ูุตุฑู ุญุงูุธู: chunk_size ุจุฒุฑฺฏ + top_k ุฒุงุฏ

ููุฒ ุจุดุชุฑ: overlap ุฒุงุฏ ุง top_k ุฒุงุฏ ุจุฏูู ููุชุฑ ููุงุณุจ

ุชูุตูโูุง ุชูุธู:
ุจุฑุง ุฏูุช ุจุงูุง:

chunk_size: 300-400

overlap: 50-80

top_k: 3-4

ุจุฑุง ุณุฑุนุช ุจุงูุง:

chunk_size: 500-600

top_k: 2-3

num_beams: 2

ุจุฑุง ุชุนุงุฏู ููุงุณุจ:

chunk_size: 450

overlap: 80

top_k: 4

num_beams: 4
</div>
