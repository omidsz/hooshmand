
# ğŸ“š RAG QA + TTS (Ø³Ù†Ø¯Ù…Ø­ÙˆØ±) â€” ØªÙˆØ¶ÛŒØ­ Ø®Ø·â€ŒØ¨Ù‡â€ŒØ®Ø· Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©
<div dir="rtl" style="text-align: right;">

Ø§ÛŒÙ† Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© ÛŒÚ© **RAG (Retrieval-Augmented Generation)** Ø³Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯:

**Ø¬Ø±ÛŒØ§Ù† Ú©Ù„ÛŒ Ú©Ø§Ø±:**

1) Ø³Ù†Ø¯ Ø±Ø§ Ø¨Ù‡ Ú†Ù†Ø¯ Â«ØªÚ©Ù‡/Ú†Ø§Ù†Ú©Â» ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†Ø¯  
2) Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú†Ø§Ù†Ú© embedding Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯  
3) Ø¯Ø§Ø®Ù„ FAISS Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù…ÛŒâ€ŒÚ©Ù†Ø¯  
4) Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ±ÛŒÙ† Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯  
5) Ø¨Ø§ Ø¢Ù†â€ŒÙ‡Ø§ ÛŒÚ© prompt Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯  
6) Ø¨Ø§ ÛŒÚ© Ù…Ø¯Ù„ Ø³Ø¨Ú© (Flan-T5) Ø¬ÙˆØ§Ø¨ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯  
7) Ø¬ÙˆØ§Ø¨ Ø±Ø§ Ø¨Ø§ gTTS ØµÙˆØªÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯  
8) Ù‡Ù…Ù‡ Ø±Ø§ Ø¯Ø§Ø®Ù„ ÛŒÚ© Ø±Ø§Ø¨Ø· Gradio Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯  
</div>


## âœ… Ø³Ù„ÙˆÙ„ 1 â€” Ù†ØµØ¨ Ù¾Ú©ÛŒØ¬â€ŒÙ‡Ø§ + Ø¨Ø±Ø±Ø³ÛŒ Torch/GPU

```python
!pip -q install faiss-cpu sentence-transformers transformers accelerate gradio gTTS
````

**Ø§ÛŒÙ† Ø³Ù„ÙˆÙ„ Ù¾Ú©ÛŒØ¬â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø±Ø§ Ù†ØµØ¨ Ù…ÛŒâ€ŒÚ©Ù†Ø¯:**
<div dir="rtl" style="text-align: right;">
* `faiss-cpu`: Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ø³Ø±ÛŒØ¹ (Vector Search) Ø±ÙˆÛŒ CPU
* `sentence-transformers`: Ø³Ø§Ø®Øª embedding Ø¨Ø±Ø§ÛŒ Ù…ØªÙ†
* `transformers`, `accelerate`: Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ HuggingFace
* `gradio`: Ø³Ø§Ø®Øª UI ÙˆØ¨ Ø¨Ø±Ø§ÛŒ Ú†Øª
* `gTTS`: ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ ØµØ¯Ø§ Ø¨Ø§ Google Text-to-Speech

> `-q` ÛŒØ¹Ù†ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ù†ØµØ¨ Ú©Ù…â€ŒØ­Ø±Ùâ€ŒØªØ± Ø¨Ø§Ø´Ø¯.
</div>


```python
import torch
print('torch:', torch.__version__)
print('cuda available:', torch.cuda.is_available())
```
<div dir="rtl" style="text-align: right;">
* `torch` Ø±Ø§ ÙˆØ§Ø±Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
* Ù†Ø³Ø®Ù‡â€ŒÛŒ Torch Ø±Ø§ Ú†Ø§Ù¾ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
* Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ø¢ÛŒØ§ CUDA (GPU) Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø§Ø³Øª ÛŒØ§ Ù†Ù‡
</div>



## âœ… Ø³Ù„ÙˆÙ„ 2 â€” Ø§ÛŒÙ…Ù¾ÙˆØ±Øªâ€ŒÙ‡Ø§ Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ø¯Ø³ØªÚ¯Ø§Ù‡ (CPU/GPU)

```python
import os
import re
import numpy as np
import faiss
from dataclasses import dataclass
```
<div dir="rtl" style="text-align: right;">
* `os`: Ú©Ø§Ø± Ø¨Ø§ Ù…Ø³ÛŒØ±Ù‡Ø§/ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ (Ø§ÛŒÙ†Ø¬Ø§ Ø®ÛŒÙ„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ø´Ø¯Ù‡)
* `re`: regex Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…ØªÙ†
* `numpy`: Ø¢Ø±Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ (embeddingÙ‡Ø§)
* `faiss`: Ø³Ø§Ø®Øª index Ùˆ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø´Ø¨Ø§Ù‡Øª
* `dataclass`: Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ø´Ø¯Ù‡ ÙˆÙ„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ø´Ø¯Ù‡ (Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø­Ø°Ù Ø´ÙˆØ¯)
</div>


```python
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
```
<div dir="rtl" style="text-align: right;">
* `SentenceTransformer`: Ù…Ø¯Ù„ embedding
* `AutoTokenizer` Ùˆ `AutoModelForSeq2SeqLM`: ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ùˆ Ù…Ø¯Ù„ ØªÙˆÙ„ÛŒØ¯ Ù…ØªÙ† (seq2seq)
* Ø¯ÙˆØ¨Ø§Ø±Ù‡ `torch` Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ø´Ø¯Ù‡ (ØªÚ©Ø±Ø§Ø±ÛŒ Ø§Ø³Øª ÙˆÙ„ÛŒ Ù…Ø´Ú©Ù„ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÛŒâ€ŒÚ©Ù†Ø¯)
</div>
```python
from gtts import gTTS
import gradio as gr
```
<div dir="rtl" style="text-align: right;">
* `gTTS`: Ù…ØªÙ† â†’ mp3
* `gradio`: UI
</div>


```python
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
print('DEVICE:', DEVICE)
```

* Ø§Ú¯Ø± GPU Ø¨Ø§Ø´Ø¯ Ø±ÙˆÛŒ `cuda` Ù…ÛŒâ€ŒØ±ÙˆØ¯ ÙˆÚ¯Ø±Ù†Ù‡ `cpu`
* Ø¯Ø³ØªÚ¯Ø§Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ÛŒ Ø±Ø§ Ú†Ø§Ù¾ Ù…ÛŒâ€ŒÚ©Ù†Ø¯



## âœ… Ø³Ù„ÙˆÙ„ 3 â€” Ù…ØªÙ† Ø³Ù†Ø¯ + ØªØ§Ø¨Ø¹ Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ txt

```python
DOCUMENT_TEXT = """
""".strip()
```
<div dir="rtl" style="text-align: right;">

* Ù…ØªØºÛŒØ± Ø§ØµÙ„ÛŒ Ø³Ù†Ø¯
* ÙØ¹Ù„Ø§Ù‹ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª
* `.strip()` ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¨ØªØ¯Ø§/Ø§Ù†ØªÙ‡Ø§ Ø±Ø§ Ø­Ø°Ù Ù…ÛŒâ€ŒÚ©Ù†Ø¯
</div>


```python
def load_txt_from_path(path: str) -> str:
    with open(path, 'r', encoding='utf-8') as f:
        return f.read()
```
<div dir="rtl" style="text-align: right;">
* ØªØ§Ø¨Ø¹ Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Ù…ØªÙ†ÛŒ UTF-8
* `with open(...)` ÛŒØ¹Ù†ÛŒ ÙØ§ÛŒÙ„ Ø¨Ø¹Ø¯ Ø§Ø² Ø®ÙˆØ§Ù†Ø¯Ù† Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ø³ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
</div>


```python
print('Document chars:', len(DOCUMENT_TEXT))
```
<div dir="rtl" style="text-align: right;">
* ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø³Ù†Ø¯ ÙØ¹Ù„ÛŒ Ø±Ø§ Ú†Ø§Ù¾ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (Ø§ÛŒÙ†Ø¬Ø§ Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ 0)
</div>


## âœ… Ø³Ù„ÙˆÙ„ 4 â€” Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ† + Ú†Ø§Ù†Ú©â€ŒÚ©Ø±Ø¯Ù†

### 1) Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ†

```python
def normalize_text(text: str) -> str:
    text = text.replace('\u200c', ' ')  # ZWNJ
    text = re.sub(r"\s+", " ", text).strip()
    return text
```
<div dir="rtl" style="text-align: right;">
* `\u200c` Ù‡Ù…Ø§Ù† **Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡ (ZWNJ)** Ø§Ø³ØªØ› Ø¢Ù† Ø±Ø§ Ø¨Ù‡ ÙØ§ØµÙ„Ù‡ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ù…ØªÙ† ÛŒÚ©Ù†ÙˆØ§Ø®Øª Ø´ÙˆØ¯
* `re.sub(r"\s+", " ", text)`: Ù‡Ø± ØªØ¹Ø¯Ø§Ø¯ whitespace (ÙØ§ØµÙ„Ù‡/Ø®Ø· Ø¬Ø¯ÛŒØ¯/ØªØ¨) â†’ ÛŒÚ© ÙØ§ØµÙ„Ù‡
* `strip()`: Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¨ØªØ¯Ø§ Ùˆ Ø§Ù†ØªÙ‡Ø§
</div>


### 2) Ú†Ø§Ù†Ú©â€ŒÚ©Ø±Ø¯Ù† Ù…ØªÙ†

```python
def chunk_text(text: str, chunk_size: int = 450, overlap: int = 80):
    """Chunking Ø³Ø§Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ø±Ø§Ú©ØªØ±
    chunk_size Ùˆ overlap Ù‚Ø§Ø¨Ù„ ØªØºÛŒÛŒØ± Ù‡Ø³ØªÙ†Ø¯
    """
```
<div dir="rtl" style="text-align: right;">
* ØªÚ©Ù‡â€ŒØªÚ©Ù‡ Ú©Ø±Ø¯Ù† Ù…ØªÙ† Ø¨Ø± Ø§Ø³Ø§Ø³ **ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ø±Ø§Ú©ØªØ±**
* `chunk_size`: Ø·ÙˆÙ„ Ù‡Ø± ØªÚ©Ù‡
* `overlap`: Ù‡Ù…Ù¾ÙˆØ´Ø§Ù†ÛŒ Ø¨ÛŒÙ† ØªÚ©Ù‡â€ŒÙ‡Ø§ (Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ù…Ø·Ø§Ù„Ø¨ Ù…Ø±Ø²ÛŒ Ø§Ø² Ø¯Ø³Øª Ù†Ø±ÙˆÙ†Ø¯)
</div>


```python
    text = normalize_text(text)
    if not text:
        return []
```
<div dir="rtl" style="text-align: right;">
* Ø§Ø¨ØªØ¯Ø§ Ù…ØªÙ† Ø±Ø§ Ù†Ø±Ù…Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
* Ø§Ú¯Ø± Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯ Ù„ÛŒØ³Øª Ø®Ø§Ù„ÛŒ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯
</div>


```python
    chunks = []
    start = 0
    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
```
<div dir="rtl" style="text-align: right;">
* Ø§Ø² `start` ØªØ§ `end` ÛŒÚ© Ø¨Ø±Ø´ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯
* `min` Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø§Ø² Ø·ÙˆÙ„ Ù…ØªÙ† Ø¬Ù„ÙˆØªØ± Ù†Ø±ÙˆØ¯
* Ø§Ú¯Ø± ØªÚ©Ù‡ Ø®Ø§Ù„ÛŒ Ù†Ø¨ÙˆØ¯ Ø¨Ù‡ Ù„ÛŒØ³Øª Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
</div>


```python
        start = end - overlap
        if start < 0:
            start = 0
        if end == len(text):
            break
```
<div dir="rtl" style="text-align: right;">
* Ø¨Ø±Ø§ÛŒ Ú†Ø§Ù†Ú© Ø¨Ø¹Ø¯ÛŒØŒ `start` Ø±Ø§ Ø¹Ù‚Ø¨â€ŒØªØ± Ù…ÛŒâ€ŒØ¨Ø±Ø¯ ØªØ§ overlap Ø§ÛŒØ¬Ø§Ø¯ Ø´ÙˆØ¯
* Ø§Ú¯Ø± Ù…Ù†ÙÛŒ Ø´Ø¯ ØµÙØ± Ù…ÛŒâ€ŒÚ©Ù†Ø¯
* Ø§Ú¯Ø± Ø¨Ù‡ Ø§Ù†ØªÙ‡Ø§ÛŒ Ù…ØªÙ† Ø±Ø³ÛŒØ¯ÛŒÙ…ØŒ Ø­Ù„Ù‚Ù‡ ØªÙ…Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯
</div>

```python
    return chunks
```

* Ø®Ø±ÙˆØ¬ÛŒ: Ù„ÛŒØ³Øª Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§



```python
chunks = chunk_text(DOCUMENT_TEXT, chunk_size=450, overlap=80)
print('Num chunks:', len(chunks))
print('Sample chunk:\n', chunks[0][:300] if chunks else 'EMPTY')
```
<div dir="rtl" style="text-align: right;">
* Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒ Ø³Ù†Ø¯ ÙØ¹Ù„ÛŒ Ø±Ø§ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯
* ØªØ¹Ø¯Ø§Ø¯Ø´Ø§Ù† Ø±Ø§ Ú†Ø§Ù¾ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
* Ø§Ú¯Ø± Ú†Ø§Ù†Ú© ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´Øª Û³Û°Û° Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§ÙˆÙ„ Ú†Ø§Ù†Ú© Ø§ÙˆÙ„ Ø±Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
</div>

## âœ… Ø³Ù„ÙˆÙ„ 5 â€” embedding Ú¯Ø±ÙØªÙ† Ø§Ø² Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§

```python
EMBED_MODEL_NAME = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
embedder = SentenceTransformer(EMBED_MODEL_NAME)
```
<div dir="rtl" style="text-align: right;">
* Ø§Ø³Ù… Ù…Ø¯Ù„ embedding Ú†Ù†Ø¯Ø²Ø¨Ø§Ù†Ù‡ (Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ù‡Ù… Ù…Ù†Ø§Ø³Ø¨)
* Ù…Ø¯Ù„ Ø±Ø§ Ù„ÙˆØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
</div>


```python
def embed_texts(texts):
    vecs = embedder.encode(texts, convert_to_numpy=True, show_progress_bar=True, normalize_embeddings=True)
    return vecs.astype('float32')
```
<div dir="rtl" style="text-align: right;">
* `encode`: Ù…ØªÙ†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø± ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
* `convert_to_numpy=True`: Ø®Ø±ÙˆØ¬ÛŒ numpy array
* `show_progress_bar=True`: Ù†Ù…Ø§ÛŒØ´ Ù†ÙˆØ§Ø± Ù¾ÛŒØ´Ø±ÙØª
* `normalize_embeddings=True`: Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ (Ø¨Ø±Ø§ÛŒ Ø´Ø¨Ø§Ù‡Øª Ø¨Ù‡ØªØ±)
* `astype('float32')`: FAISS Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ float32 Ø¯ÙˆØ³Øª Ø¯Ø§Ø±Ø¯
</div>


```python
chunk_embeddings = embed_texts(chunks) if chunks else np.zeros((0, 384), dtype='float32')
print('Embeddings shape:', chunk_embeddings.shape)
```
<div dir="rtl" style="text-align: right;">

* Ø§Ú¯Ø± Ú†Ø§Ù†Ú© Ø¯Ø§Ø±ÛŒÙ… embedding Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯
* Ø§Ú¯Ø± Ù†Ø¯Ø§Ø±ÛŒÙ… Ø¢Ø±Ø§ÛŒÙ‡ Ø®Ø§Ù„ÛŒ Ø¨Ø§ Ø´Ú©Ù„ `(0, 384)` Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯ (Û³Û¸Û´ Ø§Ø¨Ø¹Ø§Ø¯ Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø§Ø³Øª)
* Ø´Ú©Ù„ embeddingÙ‡Ø§ Ø±Ø§ Ú†Ø§Ù¾ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
</div>


## âœ… Ø³Ù„ÙˆÙ„ 6 â€” Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ FAISS

```python
def build_faiss_index(embeddings: np.ndarray):
    if embeddings.size == 0:
        return None
```

* Ø§Ú¯Ø± embedding Ø®Ø§Ù„ÛŒ Ø¨Ø§Ø´Ø¯ØŒ index Ø³Ø§Ø®ØªÙ‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯



```python
    dim = embeddings.shape[1]
    index = faiss.IndexFlatIP(dim)
    index.add(embeddings)
    return index
```
<div dir="rtl" style="text-align: right;">
* `dim`: ØªØ¹Ø¯Ø§Ø¯ Ø§Ø¨Ø¹Ø§Ø¯ Ø¨Ø±Ø¯Ø§Ø±
* `IndexFlatIP`: Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø³Ø§Ø¯Ù‡ Ø¨Ø§ Ù…Ø¹ÛŒØ§Ø± **Inner Product**
  Ú†ÙˆÙ† embeddingÙ‡Ø§ Ù†Ø±Ù…Ø§Ù„ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ØŒ `inner product â‰ˆ cosine similarity`
* `add`: Ù‡Ù…Ù‡ embeddingÙ‡Ø§ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
</div>


```python
index = build_faiss_index(chunk_embeddings)
print('FAISS index ready:', index is not None)
```

* Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø³Ø§Ø®ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡ Ø¨ÙˆØ¯Ù†Ø´ Ú†Ø§Ù¾ Ù…ÛŒâ€ŒØ´ÙˆØ¯



## âœ… Ø³Ù„ÙˆÙ„ 7 â€” Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Top-k Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø·

```python
def retrieve_top_k(query: str, k: int = 4):
    if index is None or not chunks:
        return []
```

* Ø§Ú¯Ø± Ø§ÛŒÙ†Ø¯Ú©Ø³/Ú†Ø§Ù†Ú© Ù†Ø¯Ø§Ø±ÛŒÙ…ØŒ Ø®Ø±ÙˆØ¬ÛŒ Ø®Ø§Ù„ÛŒ



```python
    q_emb = embed_texts([query])
    scores, ids = index.search(q_emb, k)
```
<div dir="rtl" style="text-align: right;">
* embedding Ø³ÙˆØ§Ù„ Ø±Ø§ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯ (ÛŒÚ© Ø³ÙˆØ§Ù„ â†’ ÛŒÚ© Ø¨Ø±Ø¯Ø§Ø±)
* `search`: Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ±ÛŒÙ† `k` Ø¨Ø±Ø¯Ø§Ø± Ø±Ø§ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯

  * `ids`: Ø§Ù†Ø¯ÛŒØ³ Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§
  * `scores`: Ø§Ù…ØªÛŒØ§Ø² Ø´Ø¨Ø§Ù‡Øª
</div>


```python
    ids = ids[0].tolist()
    scores = scores[0].tolist()
```
<div dir="rtl" style="text-align: right;">
* Ú†ÙˆÙ† Ø®Ø±ÙˆØ¬ÛŒ Ø¯ÙˆØ¨Ø¹Ø¯ÛŒ Ø§Ø³Øª (batch)ØŒ Ø³Ø·Ø± Ø§ÙˆÙ„ Ø±Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ùˆ Ø¨Ù‡ Ù„ÛŒØ³Øª ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
</div>


```python
    results = []
    for i, s in zip(ids, scores):
        if i == -1:
            continue
        results.append((chunks[i], float(s), i))
    return results
```
<div dir="rtl" style="text-align: right;">
* Ø§Ú¯Ø± `-1` Ø¨ÙˆØ¯ Ù†ØªÛŒØ¬Ù‡ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª
* Ø®Ø±ÙˆØ¬ÛŒ Ù‡Ø± Ù†ØªÛŒØ¬Ù‡: `(Ù…ØªÙ† Ú†Ø§Ù†Ú©ØŒ Ø§Ù…ØªÛŒØ§Ø²ØŒ Ø§Ù†Ø¯ÛŒØ³)`
</div>


```python
test_q = "Ù…ÙˆØ¶ÙˆØ¹ Ø³Ù†Ø¯ Ú†ÛŒØ³ØªØŸ"
print(retrieve_top_k(test_q, k=3)[:1])
```
<div dir="rtl" style="text-align: right;">
* ØªØ³Øª: Ø³ÙˆØ§Ù„ Ù…ÛŒâ€ŒÙ¾Ø±Ø³Ø¯ Ùˆ Û± Ù†ØªÛŒØ¬Ù‡ Ø§ÙˆÙ„ Ø±Ø§ Ú†Ø§Ù¾ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
</div>

<div dir="rtl" style="text-align: right;">
## âœ… Ø³Ù„ÙˆÙ„ 8 â€” Ø³Ø§Ø®Øª prompt Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ

</div>

```python
def build_prompt(context_chunks, question: str) -> str:
    context = "\n\n".join([f"[{i}] {c}" for i, c in enumerate(context_chunks, start=1)])
```

* Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨â€ŒØ´Ø¯Ù‡ Ø±Ø§ Ø´Ù…Ø§Ø±Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ùˆ ÛŒÚ©Ù¾Ø§Ø±Ú†Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯



```python
    prompt = f"""
You are a QA assistant.

RULES:
1) Answer ONLY using the provided CONTEXT.
2) If the answer is not in the context, say exactly: "Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ Ø¯Ø± Ù…ØªÙ† Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª."
3) Keep the answer concise and well-structured.

CONTEXT:
{context}

QUESTION:
{question}

ANSWER (in Persian):
""".strip()
```
<div dir="rtl" style="text-align: right;">
* prompt Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø§Ø³Øª ÙˆÙ„ÛŒ Ù…ÛŒâ€ŒÚ¯ÙˆÛŒØ¯ Ø¬ÙˆØ§Ø¨ **ÙØ§Ø±Ø³ÛŒ** Ø¨Ø§Ø´Ø¯
* Ù‚Ø§Ù†ÙˆÙ† Ù…Ù‡Ù…: ÙÙ‚Ø· Ø§Ø² `CONTEXT` Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ø› Ø§Ú¯Ø± Ù†Ø¨ÙˆØ¯ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ù‡Ù…Ø§Ù† Ø¬Ù…Ù„Ù‡ Ø±Ø§ Ø¨Ú¯Ùˆ

</div>


```python
    return prompt
```
<div dir="rtl" style="text-align: right;">
* prompt Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯
</div>


## âœ… Ø³Ù„ÙˆÙ„ 9 â€” Ù„ÙˆØ¯ Ù…Ø¯Ù„ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø® + ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®

```python
LLM_NAME = "google/flan-t5-small"
tokenizer = AutoTokenizer.from_pretrained(LLM_NAME)
model = AutoModelForSeq2SeqLM.from_pretrained(LLM_NAME)
model.to(DEVICE)
```
<div dir="rtl" style="text-align: right;">
* Ù…Ø¯Ù„ Ø³Ø¨Ú© `flan-t5-small` Ø±Ø§ Ù„ÙˆØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
* ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± Ùˆ Ù…Ø¯Ù„ Ø±Ø§ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯
* Ø±ÙˆÛŒ CPU ÛŒØ§ GPU Ù…ÛŒâ€ŒØ¨Ø±Ø¯
</div>


```python
def generate_answer(prompt: str, max_new_tokens: int = 180):
    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=1024).to(DEVICE)
```
<div dir="rtl" style="text-align: right;">
* prompt Ø±Ø§ ØªÙˆÚ©Ù†Ø§ÛŒØ² Ù…ÛŒâ€ŒÚ©Ù†Ø¯
* `truncation=True`: Ø§Ú¯Ø± Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø´Ø¯ Ù‚Ø·Ø¹ Ú©Ù†Ø¯
* `max_length=1024`: Ø³Ù‚Ù Ø·ÙˆÙ„ ÙˆØ±ÙˆØ¯ÛŒ
* Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø±ÙˆÛŒ `DEVICE` Ù…ÛŒâ€ŒØ¨Ø±Ø¯
</div>


```python
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            num_beams=4,
            do_sample=False,
        )
```
<div dir="rtl" style="text-align: right;">
* `no_grad`: inference Ø¨Ø¯ÙˆÙ† Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†
* `generate`:

  * `max_new_tokens`: Ø­Ø¯Ø§Ú©Ø«Ø± Ø·ÙˆÙ„ Ø¬ÙˆØ§Ø¨
  * `num_beams=4`: beam search Ø¨Ø±Ø§ÛŒ Ø¬ÙˆØ§Ø¨ Ø¨Ù‡ØªØ±
  * `do_sample=False`: ØªØµØ§Ø¯ÙÛŒ Ù†ÛŒØ³Øª (Ù¾Ø§ÛŒØ¯Ø§Ø±ØªØ±)
</div>


```python
    text = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()
    return text
```

* ØªØ¨Ø¯ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ø¨Ù‡ Ù…ØªÙ† Ùˆ Ø¨Ø±Ú¯Ø´Øª Ø¯Ø§Ø¯Ù† Ø¬ÙˆØ§Ø¨



### ØªØ³Øª Ø³Ø±ÛŒØ¹

```python
if chunks:
    r = retrieve_top_k("Ø³Ù†Ø¯ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ú†ÛŒØ³ØªØŸ", k=4)
    ctx = [x[0] for x in r]
    p = build_prompt(ctx, "Ø³Ù†Ø¯ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ú†ÛŒØ³ØªØŸ")
    print(generate_answer(p))
else:
    print('Document is empty. Paste or upload a .txt first.')
```
<div dir="rtl" style="text-align: right;">

* Ø§Ú¯Ø± Ø³Ù†Ø¯ Ø¯Ø§Ø±ÛŒÙ…: retrieval â†’ prompt â†’ Ø¬ÙˆØ§Ø¨ Ú†Ø§Ù¾ Ù…ÛŒâ€ŒØ´ÙˆØ¯
* Ø§Ú¯Ø± Ù†Ø¯Ø§Ø±ÛŒÙ…: Ù¾ÛŒØ§Ù… Ø³Ù†Ø¯ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª
</div>


## âœ… Ø³Ù„ÙˆÙ„ 10 â€” Ù…ØªÙ† Ø¨Ù‡ ØµØ¯Ø§ (TTS)

```python
def text_to_speech(text, out_path="answer.mp3", lang="en"):
    text = (text or "").strip()
    if not text:
        return None
```
<div dir="rtl" style="text-align: right;">
* Ù…ØªÙ† Ø±Ø§ Ø§Ù…Ù† Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (Ø§Ú¯Ø± `None` Ø¨ÙˆØ¯ØŒ Ø±Ø´ØªÙ‡ Ø®Ø§Ù„ÛŒ)
* Ø§Ú¯Ø± Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯ `None` Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯
</div>


```python
    gTTS(text=text, lang=lang).save(out_path)
    return out_path
```
<div dir="rtl" style="text-align: right;">
* Ø¨Ø§ gTTS ÙØ§ÛŒÙ„ mp3 Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯ Ùˆ Ù…Ø³ÛŒØ±Ø´ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯
</div>


## âœ… Ø³Ù„ÙˆÙ„ 11â€” ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ RAG

```python
def rag_answer(question: str, top_k: int = 4):
    if not DOCUMENT_TEXT.strip() or not chunks:
        return "Ø§Ø¨ØªØ¯Ø§ Ù…ØªÙ† Ø³Ù†Ø¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ (Paste ÛŒØ§ ÙØ§ÛŒÙ„ .txt).", []
```
<div dir="rtl" style="text-align: right;">
* Ø§Ú¯Ø± Ø³Ù†Ø¯/Ú†Ø§Ù†Ú© Ù†Ø¯Ø§Ø±ÛŒÙ…: Ù¾ÛŒØ§Ù… Ø®Ø·Ø§ + Ù„ÛŒØ³Øª Ø®Ø§Ù„ÛŒ
</div>


```python
    retrieved = retrieve_top_k(question, k=top_k)
    context_chunks = [c for (c, s, idx) in retrieved]
    prompt = build_prompt(context_chunks, question)
    answer = generate_answer(prompt)
    return answer, retrieved
```
<div dir="rtl" style="text-align: right;">

* retrieval Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
* ÙÙ‚Ø· Ù…ØªÙ† Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ Ø±Ø§ Ø¬Ø¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
* prompt Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯
* Ø¬ÙˆØ§Ø¨ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
* Ø®Ø±ÙˆØ¬ÛŒ: `(answer, retrieved_details)`
</div>


## âœ… Ø³Ù„ÙˆÙ„ 12 â€” Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ pipeline + Ø±Ø§Ø¨Ø· Gradio

### 1) Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø§ Ø³Ù†Ø¯ Ø¬Ø¯ÛŒØ¯

```python
def rebuild_pipeline_with_new_doc(doc_text: str, chunk_size: int = 450, overlap: int = 80):
    global DOCUMENT_TEXT, chunks, chunk_embeddings, index
```

* Ú†ÙˆÙ† Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ø¯ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø³Ø±Ø§Ø³Ø±ÛŒ Ø±Ø§ ØªØºÛŒÛŒØ± Ø¯Ù‡Ø¯ØŒ `global` Ù…ÛŒâ€ŒÚ¯Ø°Ø§Ø±Ø¯



```python
    DOCUMENT_TEXT = (doc_text or "").strip()
    chunks = chunk_text(DOCUMENT_TEXT, chunk_size=chunk_size, overlap=overlap)
```

* Ù…ØªÙ† Ø³Ù†Ø¯ Ø±Ø§ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
* Ú†Ø§Ù†Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯



```python
    if chunks:
        chunk_embeddings = embed_texts(chunks)
        index = build_faiss_index(chunk_embeddings)
    else:
        chunk_embeddings = np.zeros((0, 384), dtype='float32')
        index = None
    return f"âœ… Ø³Ù†Ø¯ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯. ØªØ¹Ø¯Ø§Ø¯ chunk: {len(chunks)}"
```
<div dir="rtl" style="text-align: right;">
* Ø§Ú¯Ø± Ú†Ø§Ù†Ú© Ù‡Ø³Øª: embedding â†’ index
* Ø§Ú¯Ø± Ù†ÛŒØ³Øª: Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø®Ø§Ù„ÛŒ
* Ù¾ÛŒØ§Ù… ÙˆØ¶Ø¹ÛŒØª Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯
</div>


### 2) Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Ø¢Ù¾Ù„ÙˆØ¯ÛŒ Gradio

```python
def read_uploaded_file(file_obj):
    if file_obj is None:
        return ""
```
<div dir="rtl" style="text-align: right;">
Ø³Ù¾Ø³ Ù†ÙˆØ¹â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„ÙÛŒ Ú©Ù‡ Gradio Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯ Ø±Ø§ Ù¾ÙˆØ´Ø´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯:
</div>

```python
    if isinstance(file_obj, str):
        path = file_obj
    elif hasattr(file_obj, "name"):
        path = file_obj.name
    elif isinstance(file_obj, dict) and "path" in file_obj:
        path = file_obj["path"]
    elif hasattr(file_obj, "path"):
        path = file_obj.path
    else:
        raise TypeError(...)
```

Ø¨Ø¹Ø¯ ÙØ§ÛŒÙ„ Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯:

```python
    with open(path, "r", encoding="utf-8") as f:
        return f.read()
```


<div dir="rtl" style="text-align: right;">
### 3) ØªØ§Ø¨Ø¹ Ú†Øª (ÙˆØ±ÙˆØ¯ÛŒ Ú©Ø§Ø±Ø¨Ø± â†’ Ø¬ÙˆØ§Ø¨ + Ø³ÙˆØ±Ø³â€ŒÙ‡Ø§ + ØµÙˆØª)
</div>

```python
def chat_fn(message, history, top_k, chunk_size, overlap):
    if not (DOCUMENT_TEXT and DOCUMENT_TEXT.strip()):
        return (history or []), "âŒ Ø§Ø¨ØªØ¯Ø§ Ø³Ù†Ø¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯.", None
```
<div dir="rtl" style="text-align: right;">
* Ø§Ú¯Ø± Ø³Ù†Ø¯ Ù†ÛŒØ³ØªØŒ Û³ Ø®Ø±ÙˆØ¬ÛŒ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú†ÙˆÙ† UI Ø³Ù‡ Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ø§Ø±Ø¯:

  1. history Ú†Øª
  2. markdown Ø³ÙˆØ±Ø³â€ŒÙ‡Ø§
  3. audio (Ù‡ÛŒÚ†ÛŒ)

</div>


```python
    answer, retrieved = rag_answer(message, top_k=int(top_k))
    answer = (answer or "").strip()
```
* Ø¬ÙˆØ§Ø¨ RAG
* ØªÙ…ÛŒØ²Ú©Ø§Ø±ÛŒ



```python
    if not answer:
        answer = "Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ù¾Ø§Ø³Ø® Ù‚Ø§Ø¨Ù„ ØªÙˆÙ„ÛŒØ¯ Ù†ÛŒØ³Øª..."
```

* Ø§Ú¯Ø± Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯ Ù¾ÛŒØ§Ù… fallback Ù…ÛŒâ€ŒØ¯Ù‡Ø¯



```python
    audio_path = text_to_speech(answer, out_path="answer.mp3", lang="en")
```

* Ø¬ÙˆØ§Ø¨ Ø±Ø§ ØµÙˆØªÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯

> Ø¨Ù‡ØªØ±: `lang="fa"` Ø§Ú¯Ø± ÙØ§Ø±Ø³ÛŒ Ø§Ø³Øª



```python
    sources_md = "\n\n".join([
        f"**Chunk #{idx} | score={score:.3f}**\n\n{chunk[:700]}"
        for (chunk, score, idx) in retrieved
    ])
```

* Ú†Ø§Ù†Ú©â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒâ€ŒØ´Ø¯Ù‡ Ø±Ø§ Ø¨Ù‡ markdown ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (ØªØ§ Û·Û°Û° Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§Ø² Ù‡Ø± Ú†Ø§Ù†Ú©)



```python
    history = (history or []) + [(message, answer)]
    return history, sources_md, audio_path
```

* history Ø±Ø§ Ø¢Ù¾Ø¯ÛŒØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯



### 4) Ø³Ø§Ø®Øª UI Ø¨Ø§ Gradio

```python
with gr.Blocks() as demo:
    gr.Markdown("# RAG QA + TTS (Ø³Ù†Ø¯Ù…Ø­ÙˆØ±)")
```

* ÛŒÚ© Ø§Ù¾ Gradio Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯

**Ø¨Ø®Ø´ ÙˆØ±ÙˆØ¯ÛŒ Ø³Ù†Ø¯:**

* `doc_paste = gr.Textbox(...)`
* `doc_file = gr.File(...)`
* `chunk_size = gr.Slider(...)`
* `overlap = gr.Slider(...)`
* `load_btn = gr.Button(...)`
* `load_status = gr.Textbox(...)`

**Ø¨Ø®Ø´ Ú†Øª:**

* `chatbot = gr.Chatbot(...)`
* `msg = gr.Textbox(...)`
* `top_k = gr.Slider(...)`
* `sources = gr.Markdown(...)`
* `audio = gr.Audio(..., type="filepath")`


### 5) ØªØ§Ø¨Ø¹ `on_load` (ÙˆÙ‚ØªÛŒ paste/file/slider ØªØºÛŒÛŒØ± Ú©Ù†Ø¯)

```python
def on_load(doc_text, file_obj, chunk_size, overlap):
  if doc_text and doc_text.strip():
      chosen_text = doc_text
  elif file_obj is not None:
      chosen_text = read_uploaded_file(file_obj)
  else:
      chosen_text = ""
```
<div dir="rtl" style="text-align: right;">
* Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§ paste Ø§Ø³ØªØ› Ø§Ú¯Ø± Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯ Ø§Ø² ÙØ§ÛŒÙ„ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†Ø¯
</div>


```python
  if not (chosen_text and chosen_text.strip()):
      return "âŒ Ø³Ù†Ø¯ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª..."
  return rebuild_pipeline_with_new_doc(chosen_text, int(chunk_size), int(overlap))
```

* Ø§Ú¯Ø± Ø³Ù†Ø¯ Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯ Ù¾ÛŒØ§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
* ÙˆÚ¯Ø±Ù†Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø±Ø§ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯



### 6) Ø§ØªØµØ§Ù„ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ (Event Handlers)

```python
load_btn.click(on_load, inputs=[...], outputs=[load_status])
doc_paste.change(on_load, ...)
doc_file.change(on_load, ...)
chunk_size.change(on_load, ...)
overlap.change(on_load, ...)
```

* Ø¨Ø§ Ù‡Ø± ØªØºÛŒÛŒØ±ØŒ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø³Ø§Ø®ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯


**Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù… Ú†Øª:**

```python
msg.submit(chat_fn, inputs=[msg, chatbot, top_k, chunk_size, overlap], outputs=[chatbot, sources, audio])
msg.submit(lambda: "", None, msg)
```
<div dir="rtl" style="text-align: right;">
* submit Ø§ÙˆÙ„: Ø¬ÙˆØ§Ø¨ Ø±Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯
* submit Ø¯ÙˆÙ…: textbox Ù¾ÛŒØ§Ù… Ø±Ø§ Ø®Ø§Ù„ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
</div>


### Ø§Ø¬Ø±Ø§ÛŒ Ø§Ù¾

```python
demo.launch(share=True, debug=True)
```

* Ø§Ù¾ Ø±Ø§ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
* `share=True`: Ù„ÛŒÙ†Ú© Ø¹Ù…ÙˆÙ…ÛŒ Ù…ÙˆÙ‚Øª Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
* `debug=True`: Ø®Ø·Ø§Ù‡Ø§ Ø±Ø§ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ú†Ø§Ù¾ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
