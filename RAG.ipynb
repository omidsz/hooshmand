{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYu-0p-Sjspd"
      },
      "outputs": [],
      "source": [
        "!pip -q install faiss-cpu sentence-transformers transformers accelerate gradio gTTS\n",
        "\n",
        "import torch\n",
        "print('torch:', torch.__version__)\n",
        "print('cuda available:', torch.cuda.is_available())"
      ],
      "id": "vYu-0p-Sjspd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMX5tccdjspd"
      },
      "source": [
        "## 2) ایمپورت‌ها و تنظیمات"
      ],
      "id": "IMX5tccdjspd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dB7uKH_Wjspe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import faiss\n",
        "from dataclasses import dataclass\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "\n",
        "from gtts import gTTS\n",
        "import gradio as gr\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('DEVICE:', DEVICE)\n"
      ],
      "id": "dB7uKH_Wjspe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE3f2q3Wjspe"
      },
      "outputs": [],
      "source": [
        "DOCUMENT_TEXT = \"\"\"\n",
        "\"\"\".strip()\n",
        "\n",
        "def load_txt_from_path(path: str) -> str:\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "print('Document chars:', len(DOCUMENT_TEXT))\n"
      ],
      "id": "rE3f2q3Wjspe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AtFxhD8jspe"
      },
      "source": [
        "## 4) Chunking متن با overlap\n",
        "\n",
        "در اینجا متن را به بخش‌های هم‌پوشان (Overlapping chunks) تبدیل می‌کنیم تا بازیابی دقیق‌تر شود."
      ],
      "id": "1AtFxhD8jspe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eun2E5Ntjspe"
      },
      "outputs": [],
      "source": [
        "def normalize_text(text: str) -> str:\n",
        "    text = text.replace('\\u200c', ' ')  # ZWNJ\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 450, overlap: int = 80):\n",
        "    \"\"\"Chunking ساده بر اساس تعداد کاراکتر\n",
        "    chunk_size و overlap قابل تغییر هستند\n",
        "    \"\"\"\n",
        "    text = normalize_text(text)\n",
        "    if not text:\n",
        "        return []\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = min(start + chunk_size, len(text))\n",
        "        chunk = text[start:end].strip()\n",
        "        if chunk:\n",
        "            chunks.append(chunk)\n",
        "        start = end - overlap\n",
        "        if start < 0:\n",
        "            start = 0\n",
        "        if end == len(text):\n",
        "            break\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(DOCUMENT_TEXT, chunk_size=450, overlap=80)\n",
        "print('Num chunks:', len(chunks))\n",
        "print('Sample chunk:\\n', chunks[0][:300] if chunks else 'EMPTY')\n"
      ],
      "id": "eun2E5Ntjspe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcBB4nADjspe"
      },
      "source": [
        "## 5) ساخت embedding برای chunkها\n"
      ],
      "id": "RcBB4nADjspe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPq2STRIjspe"
      },
      "outputs": [],
      "source": [
        "EMBED_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "embedder = SentenceTransformer(EMBED_MODEL_NAME)\n",
        "\n",
        "def embed_texts(texts):\n",
        "    vecs = embedder.encode(texts, convert_to_numpy=True, show_progress_bar=True, normalize_embeddings=True)\n",
        "    return vecs.astype('float32')\n",
        "\n",
        "chunk_embeddings = embed_texts(chunks) if chunks else np.zeros((0, 384), dtype='float32')\n",
        "print('Embeddings shape:', chunk_embeddings.shape)\n"
      ],
      "id": "gPq2STRIjspe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCbqcfYKjspe"
      },
      "outputs": [],
      "source": [
        "def build_faiss_index(embeddings: np.ndarray):\n",
        "    if embeddings.size == 0:\n",
        "        return None\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "index = build_faiss_index(chunk_embeddings)\n",
        "print('FAISS index ready:', index is not None)\n"
      ],
      "id": "DCbqcfYKjspe"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2uI7B6sjspe"
      },
      "source": [
        "## 7) بازیابی top-k بخش مرتبط با سوال"
      ],
      "id": "A2uI7B6sjspe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61aTQ1Snjspf"
      },
      "outputs": [],
      "source": [
        "def retrieve_top_k(query: str, k: int = 4):\n",
        "    if index is None or not chunks:\n",
        "        return []\n",
        "    q_emb = embed_texts([query])\n",
        "    scores, ids = index.search(q_emb, k)\n",
        "    ids = ids[0].tolist()\n",
        "    scores = scores[0].tolist()\n",
        "    results = []\n",
        "    for i, s in zip(ids, scores):\n",
        "        if i == -1:\n",
        "            continue\n",
        "        results.append((chunks[i], float(s), i))\n",
        "    return results\n",
        "\n",
        "test_q = \"موضوع سند چیست؟\"\n",
        "print(retrieve_top_k(test_q, k=3)[:1])\n"
      ],
      "id": "61aTQ1Snjspf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inN3AriQjspf"
      },
      "source": [
        "## 8) آماده‌سازی prompt (Context + Question + قوانین)\n",
        "\n",
        "قانون اصلی:\n",
        "\n",
        "- **Answer only from context**\n",
        "- اگر پاسخ در متن نبود، دقیقاً بگو: «اطلاعات کافی در متن موجود نیست.»\n"
      ],
      "id": "inN3AriQjspf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpnlvTe2jspf"
      },
      "outputs": [],
      "source": [
        "def build_prompt(context_chunks, question: str) -> str:\n",
        "    context = \"\\n\\n\".join([f\"[{i}] {c}\" for i, c in enumerate(context_chunks, start=1)])\n",
        "    prompt = f\"\"\"\n",
        "You are a QA assistant.\n",
        "\n",
        "RULES:\n",
        "1) Answer ONLY using the provided CONTEXT.\n",
        "2) If the answer is not in the context, say exactly: \"اطلاعات کافی در متن موجود نیست.\"\n",
        "3) Keep the answer concise and well-structured.\n",
        "\n",
        "CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION:\n",
        "{question}\n",
        "\n",
        "ANSWER (in Persian):\n",
        "\"\"\".strip()\n",
        "    return prompt\n"
      ],
      "id": "zpnlvTe2jspf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gdWV4Wbjspf"
      },
      "source": [
        "## 9) تولید پاسخ با LLM رایگان\n",
        "\n",
        "برای سازگاری چندزبانه و اجرای سبک، از `google/mt5-small` استفاده می‌کنیم.\n",
        "\n",
        "> اگر GPU دارید، سرعت بهتر خواهد بود."
      ],
      "id": "9gdWV4Wbjspf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iut0lljhjspf"
      },
      "outputs": [],
      "source": [
        "LLM_NAME = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(LLM_NAME)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(LLM_NAME)\n",
        "model.to(DEVICE)\n",
        "\n",
        "def generate_answer(prompt: str, max_new_tokens: int = 180):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True, max_length=1024).to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            num_beams=4,\n",
        "            do_sample=False,\n",
        "        )\n",
        "    text = tokenizer.decode(output_ids[0], skip_special_tokens=True).strip()\n",
        "    return text\n",
        "\n",
        "# تست سریع\n",
        "if chunks:\n",
        "    r = retrieve_top_k(\"سند درباره چیست؟\", k=4)\n",
        "    ctx = [x[0] for x in r]\n",
        "    p = build_prompt(ctx, \"سند درباره چیست؟\")\n",
        "    print(generate_answer(p))\n",
        "else:\n",
        "    print('Document is empty. Paste or upload a .txt first.')\n"
      ],
      "id": "Iut0lljhjspf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL5RANWhjspf"
      },
      "source": [
        "## 10) تبدیل پاسخ به صوت (TTS)\n",
        "\n",
        "از `gTTS` استفاده می‌کنیم (رایگان). خروجی فایل mp3 تولید می‌شود."
      ],
      "id": "UL5RANWhjspf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Va9gnCq_jspf"
      },
      "outputs": [],
      "source": [
        "from gtts import gTTS\n",
        "\n",
        "def text_to_speech(text, out_path=\"answer.mp3\", lang=\"en\"):\n",
        "    text = (text or \"\").strip()\n",
        "    if not text:\n",
        "        return None\n",
        "    gTTS(text=text, lang=lang).save(out_path)\n",
        "    return out_path\n",
        "\n"
      ],
      "id": "Va9gnCq_jspf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsJCY3Nwjspf"
      },
      "source": [
        "## 11) تابع کامل RAG (Retrieve → Prompt → Generate)\n",
        "\n",
        "این تابع تضمین می‌کند که پاسخ فقط از context تولید شود. اگر سند خالی باشد هم پیام مناسب می‌دهد."
      ],
      "id": "FsJCY3Nwjspf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLO9BkDLjspf"
      },
      "outputs": [],
      "source": [
        "def rag_answer(question: str, top_k: int = 4):\n",
        "    if not DOCUMENT_TEXT.strip() or not chunks:\n",
        "        return \"ابتدا متن سند را وارد کنید (Paste یا فایل .txt).\", []\n",
        "    retrieved = retrieve_top_k(question, k=top_k)\n",
        "    context_chunks = [c for (c, s, idx) in retrieved]\n",
        "    prompt = build_prompt(context_chunks, question)\n",
        "    answer = generate_answer(prompt)\n",
        "    return answer, retrieved\n"
      ],
      "id": "vLO9BkDLjspf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKkeCaYZjspf"
      },
      "source": [
        "## 12) رابط چت ساده با Gradio"
      ],
      "id": "OKkeCaYZjspf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kWLWPTYjspf"
      },
      "outputs": [],
      "source": [
        "def rebuild_pipeline_with_new_doc(doc_text: str, chunk_size: int = 450, overlap: int = 80):\n",
        "    global DOCUMENT_TEXT, chunks, chunk_embeddings, index\n",
        "    DOCUMENT_TEXT = (doc_text or \"\").strip()\n",
        "    chunks = chunk_text(DOCUMENT_TEXT, chunk_size=chunk_size, overlap=overlap)\n",
        "    if chunks:\n",
        "        chunk_embeddings = embed_texts(chunks)\n",
        "        index = build_faiss_index(chunk_embeddings)\n",
        "    else:\n",
        "        chunk_embeddings = np.zeros((0, 384), dtype='float32')\n",
        "        index = None\n",
        "    return f\"✅ سند بارگذاری شد. تعداد chunk: {len(chunks)}\"\n",
        "\n",
        "def read_uploaded_file(file_obj):\n",
        "    if file_obj is None:\n",
        "        return \"\"\n",
        "\n",
        "    # حالت‌های رایج در Gradio\n",
        "    if isinstance(file_obj, str):\n",
        "        path = file_obj\n",
        "    elif hasattr(file_obj, \"name\"):   # tempfile wrapper\n",
        "        path = file_obj.name\n",
        "    elif isinstance(file_obj, dict) and \"path\" in file_obj:\n",
        "        path = file_obj[\"path\"]\n",
        "    elif hasattr(file_obj, \"path\"):\n",
        "        path = file_obj.path\n",
        "    else:\n",
        "        raise TypeError(f\"Unknown file_obj type: {type(file_obj)}\")\n",
        "\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "def chat_fn(message, history, top_k, chunk_size, overlap):\n",
        "    if not (DOCUMENT_TEXT and DOCUMENT_TEXT.strip()):\n",
        "        # سه خروجی مطابق outputs\n",
        "        return (history or []), \"❌ ابتدا سند را وارد کنید.\", None\n",
        "\n",
        "    answer, retrieved = rag_answer(message, top_k=int(top_k))\n",
        "    answer = (answer or \"\").strip()\n",
        "\n",
        "    if not answer:\n",
        "        answer = \"متأسفانه پاسخ قابل تولید نیست. لطفاً سوال را واضح‌تر بپرسید یا سند را دوباره بارگذاری کنید.\"\n",
        "\n",
        "    audio_path = text_to_speech(answer, out_path=\"answer.mp3\", lang=\"en\")\n",
        "\n",
        "\n",
        "    sources_md = \"\\n\\n\".join([\n",
        "        f\"**Chunk #{idx} | score={score:.3f}**\\n\\n{chunk[:700]}\"\n",
        "        for (chunk, score, idx) in retrieved\n",
        "    ])\n",
        "    history = (history or []) + [(message, answer)]\n",
        "    return history, sources_md, audio_path\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# RAG QA + TTS (سندمحور)\")\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"## 1) ورود سند\")\n",
        "            doc_paste = gr.Textbox(label=\"متن سند (Paste)\", lines=10)\n",
        "            doc_file = gr.File(label=\"یا آپلود فایل .txt\", file_types=['.txt'])\n",
        "            chunk_size = gr.Slider(200, 1200, value=450, step=10, label=\"Chunk size (chars)\")\n",
        "            overlap = gr.Slider(0, 300, value=80, step=10, label=\"Overlap (chars)\")\n",
        "            load_btn = gr.Button(\"بارگذاری/بازسازی ایندکس\")\n",
        "            load_status = gr.Textbox(label=\"وضعیت\", interactive=False)\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"## 2) چت\")\n",
        "            chatbot = gr.Chatbot(label=\"گفتگو\")\n",
        "            msg = gr.Textbox(label=\"سوال خود را بنویسید و Enter بزنید\")\n",
        "            top_k = gr.Slider(1, 8, value=4, step=1, label=\"Top-k retrieval\")\n",
        "            sources = gr.Markdown(label=\"بخش‌های بازیابی‌شده\")\n",
        "            audio = gr.Audio(label=\"صدای پاسخ\", type=\"filepath\")\n",
        "\n",
        "    def on_load(doc_text, file_obj, chunk_size, overlap):\n",
        "    # اولویت: اگر paste پر بود همان را بگیر\n",
        "      if doc_text and doc_text.strip():\n",
        "          chosen_text = doc_text\n",
        "      # وگرنه از فایل بخوان\n",
        "      elif file_obj is not None:\n",
        "          chosen_text = read_uploaded_file(file_obj)\n",
        "      else:\n",
        "          chosen_text = \"\"\n",
        "\n",
        "      if not (chosen_text and chosen_text.strip()):\n",
        "          return \"❌ سند خالی است. متن را Paste کنید یا فایل .txt آپلود کنید.\"\n",
        "\n",
        "      return rebuild_pipeline_with_new_doc(chosen_text, int(chunk_size), int(overlap))\n",
        "\n",
        "\n",
        "    load_btn.click(on_load, inputs=[doc_paste, doc_file, chunk_size, overlap], outputs=[load_status])\n",
        "    # وقتی paste تغییر کرد => ایندکس بساز\n",
        "    doc_paste.change(\n",
        "        on_load,\n",
        "        inputs=[doc_paste, doc_file, chunk_size, overlap],\n",
        "        outputs=[load_status]\n",
        "    )\n",
        "\n",
        "    # وقتی فایل آپلود شد => ایندکس بساز\n",
        "    doc_file.change(\n",
        "        on_load,\n",
        "        inputs=[doc_paste, doc_file, chunk_size, overlap],\n",
        "        outputs=[load_status]\n",
        "    )\n",
        "    chunk_size.change(\n",
        "        on_load,\n",
        "        inputs=[doc_paste, doc_file, chunk_size, overlap],\n",
        "        outputs=[load_status]\n",
        "    )\n",
        "\n",
        "    overlap.change(\n",
        "        on_load,\n",
        "        inputs=[doc_paste, doc_file, chunk_size, overlap],\n",
        "        outputs=[load_status]\n",
        "    )\n",
        "\n",
        "\n",
        "    msg.submit(chat_fn, inputs=[msg, chatbot, top_k, chunk_size, overlap], outputs=[chatbot, sources, audio])\n",
        "    msg.submit(lambda: \"\", None, msg)\n",
        "\n",
        "demo.launch(share=True, debug=True)\n"
      ],
      "id": "6kWLWPTYjspf"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
